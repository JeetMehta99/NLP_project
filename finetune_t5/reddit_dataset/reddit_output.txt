***** Running training *****
  Num examples = 37564
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 9391
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  5%|▌         | 500/9391 [01:43<36:08,  4.10it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-500\config.json
{'loss': 2.3232, 'learning_rate': 1.8945799169417528e-05, 'epoch': 0.05}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-500\spiece.model
 11%|█         | 1000/9391 [03:28<34:55,  4.00it/s] Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-1000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-1000\config.json
{'loss': 1.1949, 'learning_rate': 1.7880949845596848e-05, 'epoch': 0.11}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-1000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-1000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-1000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-1000\spiece.model
 16%|█▌        | 1500/9391 [05:13<32:09,  4.09it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-1500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-1500\config.json
{'loss': 1.185, 'learning_rate': 1.681610052177617e-05, 'epoch': 0.16}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-1500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-1500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-1500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-1500\spiece.model
 21%|██▏       | 2000/9391 [06:59<31:45,  3.88it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-2000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-2000\config.json
{'loss': 1.1712, 'learning_rate': 1.575125119795549e-05, 'epoch': 0.21}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-2000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-2000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-2000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-2000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-500] due to args.save_total_limit
 27%|██▋       | 2500/9391 [08:44<28:19,  4.06it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-2500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-2500\config.json
{'loss': 1.1296, 'learning_rate': 1.468640187413481e-05, 'epoch': 0.27}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-2500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-2500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-2500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-2500\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-1000] due to args.save_total_limit
 32%|███▏      | 3000/9391 [10:30<26:30,  4.02it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-3000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-3000\config.json
{'loss': 1.1185, 'learning_rate': 1.3621552550314132e-05, 'epoch': 0.32}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-3000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-3000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-3000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-3000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-1500] due to args.save_total_limit
 37%|███▋      | 3500/9391 [12:15<22:05,  4.44it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-3500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-3500\config.json
{'loss': 1.1344, 'learning_rate': 1.2556703226493452e-05, 'epoch': 0.37}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-3500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-3500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-3500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-3500\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-2000] due to args.save_total_limit
 43%|████▎     | 4000/9391 [14:00<22:21,  4.02it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-4000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-4000\config.json
{'loss': 1.1302, 'learning_rate': 1.1491853902672774e-05, 'epoch': 0.43}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-4000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-4000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-4000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-4000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-2500] due to args.save_total_limit
 48%|████▊     | 4500/9391 [15:45<19:36,  4.16it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-4500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-4500\config.json
{'loss': 1.1559, 'learning_rate': 1.0427004578852094e-05, 'epoch': 0.48}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-4500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-4500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-4500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-4500\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-3000] due to args.save_total_limit
 53%|█████▎    | 5000/9391 [17:30<17:32,  4.17it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-5000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-5000\config.json
{'loss': 1.1392, 'learning_rate': 9.362155255031414e-06, 'epoch': 0.53}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-5000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-5000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-5000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-5000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-3500] due to args.save_total_limit
 59%|█████▊    | 5500/9391 [19:15<16:03,  4.04it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-5500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-5500\config.json
{'loss': 1.1228, 'learning_rate': 8.297305931210734e-06, 'epoch': 0.59}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-5500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-5500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-5500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-5500\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-4000] due to args.save_total_limit
 64%|██████▍   | 6000/9391 [21:00<13:16,  4.26it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-6000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-6000\config.json
{'loss': 1.1113, 'learning_rate': 7.232456607390055e-06, 'epoch': 0.64}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-6000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-6000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-6000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-6000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-4500] due to args.save_total_limit
 69%|██████▉   | 6500/9391 [22:45<12:03,  4.00it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-6500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-6500\config.json
{'loss': 1.1376, 'learning_rate': 6.167607283569376e-06, 'epoch': 0.69}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-6500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-6500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-6500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-6500\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-5000] due to args.save_total_limit
 75%|███████▍  | 7000/9391 [24:32<10:14,  3.89it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-7000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-7000\config.json
{'loss': 1.128, 'learning_rate': 5.102757959748697e-06, 'epoch': 0.75}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-7000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-7000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-7000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-7000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-5500] due to args.save_total_limit
 80%|███████▉  | 7500/9391 [26:17<07:41,  4.09it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-7500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-7500\config.json
{'loss': 1.1653, 'learning_rate': 4.037908635928017e-06, 'epoch': 0.8}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-7500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-7500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-7500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-7500\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-6000] due to args.save_total_limit
 85%|████████▌ | 8000/9391 [28:02<05:30,  4.21it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-8000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-8000\config.json
{'loss': 1.1085, 'learning_rate': 2.973059312107337e-06, 'epoch': 0.85}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-8000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-8000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-8000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-8000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-6500] due to args.save_total_limit
 91%|█████████ | 8500/9391 [29:48<03:41,  4.02it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-8500
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-8500\config.json
{'loss': 1.1312, 'learning_rate': 1.9082099882866576e-06, 'epoch': 0.91}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-8500\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-8500\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-8500\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-8500\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-7000] due to args.save_total_limit
 96%|█████████▌| 9000/9391 [31:33<01:36,  4.03it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\checkpoint-9000
Configuration saved in t5-small-finetuned-reddit_small\checkpoint-9000\config.json
{'loss': 1.1409, 'learning_rate': 8.454903631136195e-07, 'epoch': 0.96}
Model weights saved in t5-small-finetuned-reddit_small\checkpoint-9000\pytorch_model.bin
tokenizer config file saved in t5-small-finetuned-reddit_small\checkpoint-9000\tokenizer_config.json
Special tokens file saved in t5-small-finetuned-reddit_small\checkpoint-9000\special_tokens_map.json
Copy vocab file to t5-small-finetuned-reddit_small\checkpoint-9000\spiece.model
Deleting older checkpoint [t5-small-finetuned-reddit_small\checkpoint-7500] due to args.save_total_limit
100%|██████████| 9391/9391 [32:55<00:00,  4.77it/s]The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: content, summary. If content, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 4174
  Batch size = 4
                                                   
100%|██████████| 9391/9391 [37:02<00:00,  4.77it/s]

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 9391/9391 [37:02<00:00,  4.23it/s]{'eval_loss': 1.073380708694458, 'eval_rouge1': 15.7252, 'eval_rouge2': 2.906, 'eval_rougeL': 12.9949, 'eval_rougeLsum': 13.5809, 'eval_gen_len': 18.2063, 'eval_runtime': 246.7323, 'eval_samples_per_second': 16.917, 'eval_steps_per_second': 4.231, 'epoch': 1.0}
{'train_runtime': 2222.284, 'train_samples_per_second': 16.903, 'train_steps_per_second': 4.226, 'train_loss': 1.2041209824815648, 'epoch': 1.0}

TrainOutput(global_step=9391, training_loss=1.2041209824815648, metrics={'train_runtime': 2222.284, 'train_samples_per_second': 16.903, 'train_steps_per_second': 4.226, 'train_loss': 1.2041209824815648, 'epoch': 1.0})