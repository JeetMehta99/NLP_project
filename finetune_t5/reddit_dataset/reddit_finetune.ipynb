{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "# ! pip install datasets transformers rouge-score nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pJfZKwIgoCY3"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Apr 16 19:48:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 496.49       Driver Version: 496.49       CUDA Version: 11.5     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   54C    P0    N/A /  N/A |    121MiB /  6144MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "axiI4RQRoCY4"
      },
      "outputs": [],
      "source": [
        "# !git lfs install\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ds, info = tfds.load('reddit', split='train[:2%]', with_info=True,shuffle_files=True)\n",
        "\n",
        "# tfds.as_dataframe(ds.take(4), info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A8Vmyk3OoCY5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.17.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mnist_example = ds\n",
        "\n",
        "# list_post = []\n",
        "# list_summary = []\n",
        "\n",
        "# for sample in mnist_example:\n",
        "#   if len(sample[\"summary\"].numpy().split()) >= 10:\n",
        "#       list_post.append(sample[\"normalizedBody\"])\n",
        "#       list_summary.append(sample[\"summary\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for n in range(len(list_post)):\n",
        "#     list_post[n] = str(list_post[n].numpy(), \"utf-8\")\n",
        "#     list_summary[n] = str(list_summary[n].numpy(), \"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('train_post.txt', 'w') as f:\n",
        "#     for item in list_post:\n",
        "#         f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-169bdcdf5b78c85e\n",
            "Reusing dataset json (C:\\Users\\mishr\\.cache\\huggingface\\datasets\\json\\default-169bdcdf5b78c85e\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset('.', data_files=['train_post.txt','train_summary.txt'], split='train')\n",
        "\n",
        "dataset = load_dataset('json', data_files='C:\\\\Users\\\\mishr\\\\tensorflow_datasets\\\\downloads\\\\extracted\\\\ZIP.zeno.org_reco_1043_file_corp-webwaD4xDdMcxTTyexQ3VBTA8U2Bi2HA31NynA1uJs2k4o.zipdownload=1\\\\corpus-webis-tldr-17.json',split='train[0%:2%]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "76967"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "845"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(dataset['summary_len'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['author', 'body', 'normalizedBody', 'content', 'content_len', 'summary', 'summary_len', 'id', 'subreddit', 'subreddit_id', 'title'],\n",
              "    num_rows: 76967\n",
              "})"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I think it should be fixed on either UTC standard or UTC+1 year around, with the current zone offsets. \\n Moving timescales add a lot of complexity to the implementation of timekeeping systems and have [dubious value]( \\n I think seasonal shifting time made sense in the pre-electric past, when timekeeping was more flexible and artificial light was inefficient and often dangerous. \\n Now we have machines that work easily with simple timekeeping rules, and it's more beneficial to spend a small amount on energy for lighting, and save the larger cost of engineering things to work with the complex timekeeping rules, as well as saving the irritation to humans. \\n Lighting has gotten much more efficient over time; we can squeeze out a lot more photons per unit of energy from a 2012 CFL or LED than a candle could in 1780, or a lightbulb could in 1950. \\n There's a lot of room for improvement in how we use lights as well; as lighting control gets more intelligent, there will be a lot of savings from not illuminating inactive spaces constantly.\""
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['content'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I think it should be fixed on either UTC standard or UTC+1 year around, with the current zone offsets. \\n Moving timescales add a lot of complexity to the implementation of timekeeping systems and have [dubious value]( \\n I think seasonal shifting time made sense in the pre-electric past, when timekeeping was more flexible and artificial light was inefficient and often dangerous. \\n Now we have machines that work easily with simple timekeeping rules, and it's more beneficial to spend a small amount on energy for lighting, and save the larger cost of engineering things to work with the complex timekeeping rules, as well as saving the irritation to humans. \\n Lighting has gotten much more efficient over time; we can squeeze out a lot more photons per unit of energy from a 2012 CFL or LED than a candle could in 1780, or a lightbulb could in 1950. \\n There's a lot of room for improvement in how we use lights as well; as lighting control gets more intelligent, there will be a lot of savings from not illuminating inactive spaces constantly. \\n tl;dr: Shifting seasonal time is no longer worth it. \\n\""
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['normalizedBody'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\mishr\\.cache\\huggingface\\datasets\\json\\default-169bdcdf5b78c85e\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-e493a637c23de734.arrow\n"
          ]
        }
      ],
      "source": [
        "dataset_needed = dataset.filter(lambda example: example['summary_len'] >= 10 and example['summary_len'] <= 140)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['author', 'body', 'normalizedBody', 'content', 'content_len', 'summary', 'summary_len', 'id', 'subreddit', 'subreddit_id', 'title'],\n",
              "    num_rows: 52370\n",
              "})"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\mishr\\.cache\\huggingface\\datasets\\json\\default-169bdcdf5b78c85e\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-ec29e4ded89f57b8.arrow\n"
          ]
        }
      ],
      "source": [
        "dataset_needed = dataset_needed.filter(lambda example: example['content_len'] >= 80 and example['content_len'] <= 560)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "560"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(dataset_needed['content_len'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "updated_dataset = dataset_needed.remove_columns(['author', 'body', 'normalizedBody', 'content_len', 'summary_len', 'id', 'subreddit', 'subreddit_id', 'title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['content', 'summary'],\n",
              "    num_rows: 41738\n",
              "})"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "updated_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('train_summary.txt', 'w') as f:\n",
        "#     for item in list_summary:\n",
        "#         f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset = tf.data.Dataset.from_tensor_slices({\"document\": list_post, \"summary\": list_summary})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_from_disk\n",
        "# dataset = load_from_disk('105855_short_tokenized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['normalizedBody', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 105855\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "3oImhhdFoCY6"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"t5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "outputId": "9f8ead3b-f9fd-4571-88f7-df9264f3c189"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "# raw_datasets = load_dataset(\"xsum\")\n",
        "metric = load_metric(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at C:\\Users\\mishr/.cache\\huggingface\\transformers\\fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at C:\\Users\\mishr/.cache\\huggingface\\transformers\\65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at C:\\Users\\mishr/.cache\\huggingface\\transformers\\06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at C:\\Users\\mishr/.cache\\huggingface\\transformers\\fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "qKTn2OwFoCY_"
      },
      "outputs": [],
      "source": [
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"content\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True,padding=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split_dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:13<00:00,  3.10ba/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = updated_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenized_datasets.save_to_disk(\"1058544_tokenized_2.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# import json\n",
        "\n",
        "# with open('1058544_tokenized.txt', 'w') as convert_file:\n",
        "#      convert_file.write(json.dumps(tokenized_datasets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# short_try = short_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# short_try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenized_short_try = short_try.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenized_short_try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenized_short_try.save_to_disk(\"105855_short_tokenized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['content', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 37564\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['content', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 4174\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_tokenized_datasets['train'][7]['content'].split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_tokenized_datasets['train'][7]['summary'].split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "894"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_tokenized_datasets['train'][0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_tokenized_datasets['train'][7]['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split_dataset['validation'] = split_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# del split_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy_split_dataset = split_dataset.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenized_copy_split_dataset = split_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_split_tokenized_short_try = split_tokenized_short_try.remove_columns(['normalizedBody','summary',])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_split_tokenized_short_try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at C:\\Users\\mishr/.cache\\huggingface\\transformers\\fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\mishr/.cache\\huggingface\\transformers\\fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "ItY1F1GsoCZA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-reddit_small\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size, \n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    # push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "nsAClfXjoCZB"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['content', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 37564\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['content', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 4174\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_return = ['input_ids', 'labels', 'attention_mask']\n",
        "split_tokenized_datasets.set_format(type='torch', columns=columns_to_return)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(dataset[:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n",
            "100%|██████████| 1044/1044 [31:22<00:00,  1.80s/it]\n"
          ]
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=split_tokenized_datasets['train'],\n",
        "    eval_dataset=split_tokenized_datasets['test'],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# os.environ[\"WANDB_DISABLED\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: content, summary. If content, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "C:\\Users\\mishr\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 37564\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 9391\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "  5%|▌         | 500/9391 [01:43<36:08,  4.10it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.3232, 'learning_rate': 1.8945799169417528e-05, 'epoch': 0.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-500\\spiece.model\n",
            " 11%|█         | 1000/9391 [03:28<34:55,  4.00it/s] Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-1000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1949, 'learning_rate': 1.7880949845596848e-05, 'epoch': 0.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-1000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-1000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-1000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-1000\\spiece.model\n",
            " 16%|█▌        | 1500/9391 [05:13<32:09,  4.09it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-1500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-1500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.185, 'learning_rate': 1.681610052177617e-05, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-1500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-1500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-1500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-1500\\spiece.model\n",
            " 21%|██▏       | 2000/9391 [06:59<31:45,  3.88it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-2000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-2000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1712, 'learning_rate': 1.575125119795549e-05, 'epoch': 0.21}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-2000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-2000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-2000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-2000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-500] due to args.save_total_limit\n",
            " 27%|██▋       | 2500/9391 [08:44<28:19,  4.06it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-2500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-2500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1296, 'learning_rate': 1.468640187413481e-05, 'epoch': 0.27}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-2500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-2500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-2500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-2500\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-1000] due to args.save_total_limit\n",
            " 32%|███▏      | 3000/9391 [10:30<26:30,  4.02it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-3000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-3000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1185, 'learning_rate': 1.3621552550314132e-05, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-3000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-3000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-3000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-3000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-1500] due to args.save_total_limit\n",
            " 37%|███▋      | 3500/9391 [12:15<22:05,  4.44it/s]  Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-3500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-3500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1344, 'learning_rate': 1.2556703226493452e-05, 'epoch': 0.37}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-3500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-3500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-3500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-3500\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-2000] due to args.save_total_limit\n",
            " 43%|████▎     | 4000/9391 [14:00<22:21,  4.02it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-4000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-4000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1302, 'learning_rate': 1.1491853902672774e-05, 'epoch': 0.43}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-4000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-4000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-4000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-4000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-2500] due to args.save_total_limit\n",
            " 48%|████▊     | 4500/9391 [15:45<19:36,  4.16it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-4500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-4500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1559, 'learning_rate': 1.0427004578852094e-05, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-4500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-4500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-4500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-4500\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-3000] due to args.save_total_limit\n",
            " 53%|█████▎    | 5000/9391 [17:30<17:32,  4.17it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-5000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-5000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1392, 'learning_rate': 9.362155255031414e-06, 'epoch': 0.53}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-5000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-5000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-5000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-5000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-3500] due to args.save_total_limit\n",
            " 59%|█████▊    | 5500/9391 [19:15<16:03,  4.04it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-5500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-5500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1228, 'learning_rate': 8.297305931210734e-06, 'epoch': 0.59}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-5500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-5500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-5500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-5500\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-4000] due to args.save_total_limit\n",
            " 64%|██████▍   | 6000/9391 [21:00<13:16,  4.26it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-6000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-6000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1113, 'learning_rate': 7.232456607390055e-06, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-6000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-6000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-6000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-6000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-4500] due to args.save_total_limit\n",
            " 69%|██████▉   | 6500/9391 [22:45<12:03,  4.00it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-6500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-6500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1376, 'learning_rate': 6.167607283569376e-06, 'epoch': 0.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-6500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-6500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-6500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-6500\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-5000] due to args.save_total_limit\n",
            " 75%|███████▍  | 7000/9391 [24:32<10:14,  3.89it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-7000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-7000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.128, 'learning_rate': 5.102757959748697e-06, 'epoch': 0.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-7000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-7000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-7000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-7000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-5500] due to args.save_total_limit\n",
            " 80%|███████▉  | 7500/9391 [26:17<07:41,  4.09it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-7500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-7500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1653, 'learning_rate': 4.037908635928017e-06, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-7500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-7500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-7500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-7500\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-6000] due to args.save_total_limit\n",
            " 85%|████████▌ | 8000/9391 [28:02<05:30,  4.21it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-8000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-8000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1085, 'learning_rate': 2.973059312107337e-06, 'epoch': 0.85}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-8000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-8000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-8000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-8000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-6500] due to args.save_total_limit\n",
            " 91%|█████████ | 8500/9391 [29:48<03:41,  4.02it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-8500\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-8500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1312, 'learning_rate': 1.9082099882866576e-06, 'epoch': 0.91}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-8500\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-8500\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-8500\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-8500\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-7000] due to args.save_total_limit\n",
            " 96%|█████████▌| 9000/9391 [31:33<01:36,  4.03it/s]Saving model checkpoint to t5-small-finetuned-reddit_small\\checkpoint-9000\n",
            "Configuration saved in t5-small-finetuned-reddit_small\\checkpoint-9000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1409, 'learning_rate': 8.454903631136195e-07, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in t5-small-finetuned-reddit_small\\checkpoint-9000\\pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-reddit_small\\checkpoint-9000\\tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-reddit_small\\checkpoint-9000\\special_tokens_map.json\n",
            "Copy vocab file to t5-small-finetuned-reddit_small\\checkpoint-9000\\spiece.model\n",
            "Deleting older checkpoint [t5-small-finetuned-reddit_small\\checkpoint-7500] due to args.save_total_limit\n",
            "100%|██████████| 9391/9391 [32:55<00:00,  4.77it/s]The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: content, summary. If content, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4174\n",
            "  Batch size = 4\n",
            "                                                   \n",
            "100%|██████████| 9391/9391 [37:02<00:00,  4.77it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 9391/9391 [37:02<00:00,  4.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.073380708694458, 'eval_rouge1': 15.7252, 'eval_rouge2': 2.906, 'eval_rougeL': 12.9949, 'eval_rougeLsum': 13.5809, 'eval_gen_len': 18.2063, 'eval_runtime': 246.7323, 'eval_samples_per_second': 16.917, 'eval_steps_per_second': 4.231, 'epoch': 1.0}\n",
            "{'train_runtime': 2222.284, 'train_samples_per_second': 16.903, 'train_steps_per_second': 4.226, 'train_loss': 1.2041209824815648, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9391, training_loss=1.2041209824815648, metrics={'train_runtime': 2222.284, 'train_samples_per_second': 16.903, 'train_steps_per_second': 4.226, 'train_loss': 1.2041209824815648, 'epoch': 1.0})"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mP2pndLZoCZB"
      },
      "outputs": [],
      "source": [
        "# trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['normalizedBody', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 4174\n",
              "})"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.eval_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: content, summary. If content, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 4174\n",
            "  Batch size = 4\n",
            "100%|██████████| 1044/1044 [03:59<00:00,  4.72it/s]"
          ]
        }
      ],
      "source": [
        "predictions = trainer.predict(split_tokenized_datasets[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[    0,  1336,  6856, ...,   344,     3,     5],\n",
              "       [    0,  9825,    11, ...,   129,    91,   132],\n",
              "       [    0, 10060,   188, ...,    10,   989,    13],\n",
              "       ...,\n",
              "       [    0,  7442,    56, ...,    39,  2583,     6],\n",
              "       [    0,    27,  1971, ...,    82,   609,     1],\n",
              "       [    0,   261,     8, ...,     3, 26968,  2475]], dtype=int64), label_ids=array([[ 1336,  6856,  4094, ...,     0,     0,     0],\n",
              "       [ 9825,    11,  1705, ...,     0,     0,     0],\n",
              "       [10060,   188,  9760, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 7442,    56,  8195, ...,     0,     0,     0],\n",
              "       [   27,  1971,    12, ...,     0,     0,     0],\n",
              "       [  261,     8,  3596, ...,     0,     0,     0]], dtype=int64), metrics={'test_loss': 0.0028957556933164597, 'test_rouge1': 74.6676, 'test_rouge2': 73.5471, 'test_rougeL': 74.685, 'test_rougeLsum': 74.6753, 'test_gen_len': 18.6246, 'test_runtime': 253.794, 'test_samples_per_second': 16.446, 'test_steps_per_second': 4.114})"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4174, 20) (4174, 128)\n"
          ]
        }
      ],
      "source": [
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'As a present-day consultant and former hiring manager, I would advise against studying for or investing in anything other than current certification tracks. \\n Server 08 is a 7 year old operating system and while it might be common to find it out there, the only companies that are deploying that OS or that see value in that cert anymore (over the 2012R2 track that is) are those that are more interested in perpetuating an archaic approach to IT strategy than in adapting new tech and new trends.'"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_tokenized_datasets[\"test\"]['content'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"a server '08 cert in mid-2014 might help get you a job, but not one that is going to keep you relevant or interested in technology.\""
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_tokenized_datasets[\"test\"]['summary'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split_tokenized_datasets[\"test\"]['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "# inputs = tokenizer('The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - a charity to raise money for Nigerian sport.\\nMr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\\nAppearing at the Old Bailey earlier, all four denied the offence.\\nThe charge relates to offences which allegedly took place between 2008 and 2014.\\nSam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand trial in July.\\nThey were all released on bail.', return_tensors=\"pt\")\n",
        "# labels = torch.tensor([1]).unsqueeze(0).cuda()  # Batch size 1\n",
        "# # labels = labels.device('cuda')\n",
        "# # outputs = model(**inputs, labels=labels)\n",
        "\n",
        "# outputs = model(inputs.input_ids.cuda(),inputs.attention_mask.cuda(), labels=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tweet = \"Dejan Kulusevski has created at least 1 shooting opportunity from inside the box for his teammates in 6 consecutive Premier League games now. Not a single shot for Dejan Kulusevski today, but his streak of creating at least 1 shooting chance from inside the penalty area for his teammates increases to 5 PL games now. Dejan Kulusevski received the most number of passes amongst Spurs' front 3. Even if you ignore the high quality chance he created for Sonny, if the player playing alongside Kane and Son, passes them the ball 30 of the times and only has 3/45 unsuccessful passes, his job is done.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Dejan Kulusevski has created at least 1 shooting opportunity from inside the box for'"
            ]
          },
          "execution_count": 245,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(input_tweet, max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"].cuda())\n",
        "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 'The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - \n",
        "# a charity to raise money for Nigerian sport.\\n\n",
        "# Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\\n\n",
        "# Appearing at the Old Bailey earlier, all four denied the offence.\\nThe charge relates to offences which allegedly took place between 2008 and 2014.\\n\n",
        "# Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand trial in July.\\n\n",
        "# They were all released on bail.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics_changed(predictions, labels):\n",
        "    predictions, labels = predictions, labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "\n",
        "    \n",
        "    return decoded_preds, decoded_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "d_preds, d_labels = compute_metrics_changed(predictions.predictions, predictions.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most popular sound solutions for something like the Canon 600D are the Rode Videomic Pro and something like the Zoom H4n. In short, the onboard preamps for audio on DSLR cameras are terrible. To compensate for that, the Rode Videomic Pro is a version that has boostable, +20dB battery-powered preamp to help give you decent on-board sound. \\n Alternatively, most filmmakers will recommend that you record audio separately on a recorder such as the Zoom H4n (or Tascam DR-40) and sync them up in post because you will get much better sound. You can also buy an XLR shotgun microphone such as the Rode NTG-2 to give you better directional sound as well. \\n As for a microphone that you would stick to a person to record for a prank, you would be looking into something called a lavalier microphone. They clip onto your clothes and stay on your person. For this, you would need to buy a separate audio recorder such as the Zoom H1(~$100) and a lav mic such as the Azden EX503(~$25).'"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_tokenized_datasets[\"test\"]['content'][31]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lenghts = []\n",
        "for n in d_preds:\n",
        "    lenghts.append(len(n.split()))\n",
        "\n",
        "max(lenghts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Don't stop telling them what's right, you're making a huge difference.\""
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_labels[28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Rivera is horrendous in left field.\\nThe Jays are a great bat,'"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_preds[27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Summarization",
      "provenance": []
    },
    "interpreter": {
      "hash": "d61e1109225bf29d6c16a9bc24f38ed9db671ded9b9d09d5c99f491a5ccf2da8"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('new')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
