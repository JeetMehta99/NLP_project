{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd  \n",
    "\n",
    "#cleans tweets, if the cleaned tweet ends up being smaller than 3 words, then an empty string will be returned.\n",
    "def clean_tweet(tweet):\n",
    "    if type(tweet) == np.float64:\n",
    "        return \"\"\n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp)\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub('rt ','', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    if len(temp) < 3:\n",
    "        return \"\"\n",
    "    else:\n",
    "        temp = \" \".join(word for word in temp)\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcede03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "#load pre trained model to detect sentiment\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter filenames with tweets here\n",
    "\n",
    "all_files = ['arsenal.txt',\n",
    "'aston_vila.txt',\n",
    "'chelsea.txt',\n",
    "'everton.txt',\n",
    "'leeds.txt',\n",
    "'leicester.txt',\n",
    "'liverpool.txt',\n",
    "'man_city.txt',\n",
    "'man_united.txt',\n",
    "'newcastle.txt',\n",
    "'palace.txt',\n",
    "'spurs.txt',\n",
    "'westham.txt',\n",
    "'wolves.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8558778",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filenames in all_files:\n",
    "    # print()\n",
    "    with open(filenames.split(\".\")[0] + \".txt\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        tweet = []\n",
    "        processed_tweet = []\n",
    "        positive = []\n",
    "        negative = []\n",
    "        neutral = []\n",
    "\n",
    "        for line in lines:\n",
    "            if line != \"\\n\":\n",
    "            \n",
    "                text = line\n",
    "                text = clean_tweet(text)\n",
    "                if text.strip():\n",
    "                    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "                    output = model(**encoded_input)\n",
    "                    scores = output[0][0].detach().numpy()\n",
    "                    scores = softmax(scores)\n",
    "\n",
    "                    ranking = np.argsort(scores)\n",
    "                    ranking = ranking[::-1]\n",
    "                    for i in range(scores.shape[0]):\n",
    "                        l = labels[ranking[i]]\n",
    "                        s = scores[ranking[i]]\n",
    "                        s = np.round(float(s), 4)\n",
    "                        if l == 'positive':\n",
    "                            positive.append(s)\n",
    "                        elif l == 'negative':\n",
    "                            negative.append(s)\n",
    "                        else:\n",
    "                            neutral.append(s)\n",
    "\n",
    "                    tweet.append(line)\n",
    "                    processed_tweet.append(text)\n",
    "\n",
    "\n",
    "        # dictionary of lists  \n",
    "        dict = {'tweet': tweet, 'processed_tweet': processed_tweet, 'positive': positive, 'negative' : negative, 'neutral' : neutral}  \n",
    "            \n",
    "        df = pd.DataFrame(dict) \n",
    "                \n",
    "        # saving the dataframe \n",
    "        df.to_csv(filenames.split(\".\")[0] + '.csv') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
